<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>Kunal Handa</title>
    <link rel="stylesheet" href="tufte.css"/>
    <link rel="stylesheet" href="latex.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>

  <body>
    <article>
      <h1 id="Kunal Handa">Kunal Handa</h1>
      <section>
        <p><label for="mn-figure-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="mn-figure-1" class="margin-toggle"/><span class="marginnote"> <img src="img/me.jpg" onmouseover="this.src='img/yodie.jpg'" onmouseout="this.src='img/me.jpg'" border="0" width="400" height=auto></span></p>

        <p><span class="newthought">I am a MSc by Research student</span>  at the University of Oxford advised by <a href="https://www.cs.ox.ac.uk/people/yarin.gal/website">Yarin Gal</a> and <a href="https://sebastianfarquhar.com/">Sebastian Farquhar</a>. Previously, I was a linguistics and computer science undergrad at Brown University advised by <a href="https://cs.brown.edu/people/epavlick/">Ellie Pavlick</a> and <a href="https://chensun.me/index.html">Chen Sun</a>. I'm broadly interested in 1) exploring how foundation models can be safely and robustly deployed in complex, real-world scenarios and 2) grounding this discussion in our understanding of human cognition and value systems.</p>
        <p>I {am, have been} fortunate to work with some amazing people as a part of the <a href="https://oatml.cs.ox.ac.uk/">Oxford OATML Group</a>, <a href="https://cocolab.stanford.edu/">Stanford CoCoLab</a>, <a href="https://lunar.cs.brown.edu/#">Brown LUNAR Lab</a>, <a href="https://nlp.stanford.edu/">Stanford NLP Group</a>, <a href="https://chensun.me/">Brown PALM Lab</a>, and <a href="https://langcog.stanford.edu/">Stanford LangCog Lab</a>.</p>
        <p>You can contact me at: kunal <var>[underscore]</var> handa <var>[at]</var> alumni <var>[dot]</var> brown <var>[dot]</var> edu</p>
      </section>


      <section>
        <h2 id="Currently">Currently,</h2>
        <p>I'm exploring how to improve large language models' preference-learning abilities with <a href="https://www.alextamkin.com/">Alex Tamkin</a>, <a href="https://belindal.github.io/">Belinda Li</a>, <a href="https://cs.brown.edu/people/epavlick/">Ellie Pavlick</a>, <a href="https://cocolab.stanford.edu/ndg">Noah Goodman</a>, and <a href="https://www.mit.edu/~jda/">Jacob Andreas</a>.</p>
      </section>

      <section>
        <h2 id="Publications">Publications</h2>
        <h3 id="Conference Papers">Conference Papers</h3>
        <p><cite><a href="#">Anonymized</a></cite>
        <br><b>Kunal Handa</b>, Margarett Clapper, Jessica Boyle, Rose E Wang, Diyi Yang, David Yeager, Dorottya Demszky. Pending in Empirical Methods in Natural Language Processing (EMNLP), 2023.</p>

        <p><cite><a href="#">Anonymized</a></cite>
        <br>Tian Yun*, Zilai Zeng*, <b>Kunal Handa</b>, Ashish V Thapliyal, Bo Pang, Ellie Pavlick, Chen Sun. Pending in Empirical Methods in Natural Language Processing (EMNLP), 2023.</p>

        <p><cite><a href="https://arxiv.org/abs/2212.10711">Task Ambiguity in Humans and Language Models</a></cite>
        <br>Alex Tamkin*, <b>Kunal Handa*</b><label for="co-first" class="margin-toggle sidenote-number"></label><input type="checkbox" id="co-first" class="margin-toggle"/><span class="sidenote">* denotes equal contribution.</span>, Avash Shrestha, Noah Goodman. In the International Conference on Learning Representations (ICLR), 2023.</p>

        <h3 id="Journal Articles">Journal Articles</h3>
        <p><cite><a href="https://link.springer.com/article/10.3758/s13428-022-01906-4">Peekbank: An open, large-scale repository for developmental eye-tracking data of children’s word recognition</a></cite>
        <br>Martin Zettersten... <b>Kunal Handa</b>... & Michael C Frank. In Behavior Research Methods (BRM), 2022.</p>
      </section>

      <section>
        <h2 id="Other Writing">Other Writing</h2>
        <p><cite><a href="https://cs.brown.edu/about/conduit/">The Role of Technology in Elections: The Voyage of Voters’ Data</a></cite>
        <br><b>Kunal Handa.</b> In Conduit, the Brown University Computer Science Annual Magazine, Volume 32, 2022.</p>

        <p><cite><a href="https://docs.google.com/document/d/1poIXPXfdwpjvv5p6xtdDGGQclma0Q9eNbw3dpwKieKY/edit?usp=sharing">Racial Bias and the Loaded Language of Gun-Violence Related Reporting</a></cite>
        <br><b>Kunal Handa*</b>, Arun Chintalapati*</p>

        <p><cite><a href="https://docs.google.com/document/d/1ubSFQExPFSXXleB6fPT-4Tdg_9eS4trjVt9k-viAgHI/edit?usp=sharing">Trying to Give a Shit About <em>"Give a Shit"</em>: A Compositional Semantics Perspective</a></cite>
        <br><b>Kunal Handa</b><p>
        
      </section>

      <section>
        <h2 id="Teaching">Teaching</h2>
      <p>At Brown, I served as the Socially-Responsible Computing Teaching Assistant for CS1470: Deep Learning. I designed content<label for="STA content" class="margin-toggle sidenote-number"></label><input type="checkbox" id="STA content" class="margin-toggle"/><span class="sidenote">some of which is available via <a href="https://brown-deep-learning.github.io/dl-website-f22/assignments.html">the course's website</a></span> on deep learning’s potential societal harms and conducted exercises that examined ethical frameworks, the cyclical nature of language models' biases, and the pros and cons of regulating ML advancements.</p>

      <p>I also tutor incarcerated and previously incarcerated individuals in HiSET test preparation and essay writing. Intermittently, I teach group classes such as Applying to College and Introduction to Philosophy at juvenille detention centers.</p>
      <section>
        <h2 id="Academic Service">Academic Service</h2>
        <p>Earlier, I reviewed for Empirical Methods in Natural Language Processing (EMNLP) 2023. And, later, I will review for the Socially Responsible Language Modelling Research (SoLaR) Workshop at the Conference on Neural Information Processing Systems (NeurIPS) 2023.</p>
      </section>
    </article>
  </body>
</html>
